{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 深度学习\n",
    "\n",
    "导数就是求变化率，而偏导数则是当变量超过一个的时候，对其中一个变量求变化率。\n",
    "\n",
    "### 线性最小二乘\n",
    "\n",
    "## 机器学习看不懂公式怎么办\n",
    "\n",
    "[看机器学习论文时，看不懂数学公式怎么办？](https://www.zhihu.com/question/50967184)\n",
    "\n",
    "### 首先是基础知识方面：\n",
    "\n",
    "1. 概率与数理统计 - 本质问题\n",
    "\n",
    "我们基本可以认为当前机器学习的大部分根基都建立于概率与数理统计的理论之上，无论是NaiveBayes，HMM，概率决策树等PGM模型，抑或是支持向量机、神经网络等“现代方法”，其背后都是有庞大的概率体系作为支撑的。我们甚至可以用概率的方式去定义Generative model和Discriminative model的本质区别：\n",
    "\n",
    "> Generative Model 计算的目标是求解P(y,x)，而 Discriminiative Model的计算目标是求解P(y|x)\n",
    "\n",
    "在概率统计中着重需要熟练运用的概念有：\n",
    "\n",
    "* 贝叶斯理论：重中之重，不必阐述贝叶斯理论在各类PGM中的重要性了，在神经网络的各种模型、方法、技巧中，也存在着大量的Bayes Approach & Explanations\n",
    "\n",
    "* 随机变量的各种特征、运算规则、分布等：其实应该和上面的放在一起说，同样的，不必说BayesNetwork以及MarkovNetwork这样显而易见的模型，在当前的深度学习中，由各类分布、Factor运算、以及相应的随机假设而推导出的方法、公式也是占了大量比例的（比如Softmax function、Hopfield Network的父模型MRF背后的Boltzmann/Gibbs分布等）。\n",
    "\n",
    "* 统计方法：应用领域也很多，最典型的是对于机器学习系统Performance的评判，以及一些上层改进方法（如Cross-validation等）。\n",
    "\n",
    "* 总和上述各个方面的似然方法：大量的机器学习问题，最后都可以归为概率-统计模型的似然求解问题，无论是问题本身的构建、求解，还是相应方法的优化，这一知识体系始终贯穿整个机器学习领域。\n",
    "\n",
    "2. 线性代数 -  模型建立\n",
    "\n",
    "线性代数在机器学习中的重要性，既非“线性”也非“代数”，更**不在于很多人指出的“计算机对于矩阵运算有更高效的优化”**，而在于现代机器学习系统的大量学习方法，都可以看做**根据学习的目的**，在为数据**建立向量空间**，并对这一向量空间进行**映射变换**，而达成**直接求解或简化问题**的目的（比如使得线性不可分的数据变得可分）。无论是Linear/Logistic Regression，还是神经网络（相关的解释可以参考Neural Networks, Manifolds, and Topology）。\n",
    "\n",
    "相应地，当你把机器学习的各类问题转换为上面的视角去理解，那么在问题的建模、求解、优化中采用大量线性代数的知识，也不足为奇了。\n",
    "\n",
    "3. 高等数学 - 求解方法\n",
    "\n",
    "这一块具体怎么称呼众说纷纭，有人说要学好微积分，有人说会求导求梯度就好，有人说要把数分啃下来云云，其实在我的理解中，这些被广泛中国大学生概括为“高数”的东西，在机器学习里的主要以及重要应用就在于——**近似的艺术**。\n",
    "\n",
    "为什么这么说？因为整个机器学习领域，从Statistical inference到Deep learning，几乎一路上全是各种NP-hardness，要做精确的计算，其所需计算资源随模型复杂度是指数级上升到。所以我们需要大量的方法对模型中的运算、求解（比如凸优化等），都需要大量的近似算法来进行，从Gradient Descent逼近（相应知识体系：梯度、极值），到Truncated Newton method（又称Hessian-Free Optimization，可以参考我之前写的知乎专栏，涉及到Hessian矩阵、级数展开等），以及在计算概率分布时使用的各种逼近算法（这一块基本就是各种微积分求解了），几乎都逃不出“高数”的手掌心。\n",
    "\n",
    "除此之外，如果要走学术道路，这些内容的深化与扩展，包括但不限于：泛函分析、复分析、信息学、热力学等，对于后续的学习理解与研究启发，也是有深刻的帮助的。\n",
    "\n",
    "### 接下来是学习方法方面：\n",
    "\n",
    "这一方面我不做过于细致的展开，因为对于每个不同的个体而言，合适的学习方法千差万别，适合我自己的学习方法不一定适合这个答案的读者，所以在这一过程中，我只能蜻蜓点水般地说一说，大量展开难免夹带私货。\n",
    "\n",
    "**学习路线：**\n",
    "\n",
    "我个人比较推荐的参考学习路线是，首先有一定的了解和基础（比如Coursera上AndrewNg的机器学习课程），这一步主要是知道机器学习做什么，以及一些基本的概念、方法。\n",
    "\n",
    "然后研学PGM，看一下PGM的基础教材，并了解一下各类概率模型在感兴趣的领域中的具体应用，各种Probablistic Graphical Models包含的知识体系，视野视角，对于后序的学习都有着很高的帮助。\n",
    "\n",
    "而后开始学神经网络，对于神经网络的学习，其实我个人觉得先从Hopfield Network学起（这一块是彻底彻底的私货，仅供参考），一个是模型简单、实现方便，另一方面可以大量借助之前PGM中学习的MRF知识进行辅助，然后又HopfieldNetwork的无向图推到有向模型，而后展开到分层神经网络，结合之前入门时的梯度下降方法手推一遍BackPropagation，在接下来就可以研学神经网络的各种变种、不同模型的结合等等。这一过程，推荐通过论文而非教材/教程进行学习。\n",
    "\n",
    "**学习方式：**\n",
    "\n",
    "我比较推崇“**深度优先学习法**”，就是在学习的过程中，重视知识网的建立，遇到重要或有趣的内容，及时Dive in（但也要记得**在走太远的时候及时终止**）。\n",
    "\n",
    "比如在书、课程中遇到不熟悉/想更深了解的概念，记下来，开始进行搜索（可以从Wikipedia、Scholarpedia等进行入手，而后一并翻出各类资料来，对于重要的内容，还可以在arXiv.org等公开论文平台上获取一些有价值的论文），对于论文的研读过程，更可以在最后的Reference中寻找一些值得一看的论文。\n",
    "\n",
    "这种学习方法，除了能够对你的基础知识进行查漏补缺，对上层知识达到触类旁通，还有一个重要的作用：在**寻本朔源的过程中，你会发现正在学的某个内容，和过去自己熟知的其他内容，本质上是同源的**，很多无法理解的概念在这一瞬间，就恍然大悟了。\n",
    "\n",
    "*我自己的一个例子是在学习MRF的时候，查阅MRF Factorization与Gibbs/Boltzmann分布的关系，并且（看着统计热力学资料）手推了一遍Gibbs/Boltzmann分布，而后突然明白Softmax分类函数、以及其作为分母的Partition Funtion的根本意义，以及后来在学到Hopfield Network时，更因为在推导Boltzmann分布的时候接触了一部分的统计力学、动力学知识，而后涉及到Ising 模型、能量函数的本质时，也有了更自然的理解入口。*\n",
    "\n",
    "另当一提，对于初学来说，各类“水文”其实有着相当的参考价值（当然，不必精读），主要有以下两点：\n",
    "\n",
    "* 初学者非常容易遇到的问题，就是“道理我都明白了，可是这玩意到底干嘛用的/怎么用的？”，而许多水文，文章不长，信息量稀疏，但却能很好地作为参考，来解答新手的这一问题\n",
    "\n",
    "* 为了保证水文看起来“不那么水”，大量的水文背后都有着非常值得一看的Reference，可以挑出其中有价值的部分当ReadingList了。。。。\n",
    "\n",
    "## 机器学习中的数学\n",
    "\n",
    "[机器学习中的数学](https://www.jianshu.com/p/7956f4509ffe)\n",
    "\n",
    "### 工程界的恐慌\n",
    "无论我自己亲眼所见还是道听途说，虽然国内兴起了一段人工智能的浪潮，但是在企业内部对这个领域的了解还是比较局限的。一般来说就是两个态度: 第一种是隔岸观火，这些人认为这个东西太遥远了，都是理论公式，和实际的应用没啥关系，等他们搞的差不多了，出了一些开源库(如tensorflow)我们直接用就行了；第二种是砸锅卖铁搞科研，他们吹嘘这是第四次工业革命，用一种赌徒的心理要把员工个个培养成数学家。当然前者往往是中小型公司，后者往往是财力雄厚的大型互联网公司。\n",
    "\n",
    "这种现象的原因在哪里呢？\n",
    "\n",
    "首先，企业决策层面缺少懂行的人，数据科学不是做业务，你想让一个数据科学家对你的业务熟悉的比你的老板还要精通，这是不现实的。所以，数据科学一定是自上而下的推动。\n",
    "\n",
    "其次，企业不知道招什么样的人，大部分公司还是局限在要招能立马上手干活的，所以开出了以下条件：大数据背景，精通机器学习、深度学习算法，编程大牛。虽然开出的薪资十分诱人，但是达标的很少。为什么呢，我们来看看这个条件，做大数据的人一般都是在做sql的查询优化聚合计算，做机器学习的人一般都是高校的研究生和博士，编程大牛基本上都是在社会上打拼过一段时间的码农。试想要在这三个集合中求一个交集找到的只能是有工作经验、高学历背景而且工作后还能孜孜不倦自我提升的人。在这个物欲横流的社会，找到这样的人基本是一个小概率事件。\n",
    "\n",
    "最后，这也是最重要的，工程师对机器学习的恐惧。很多已经毕业多年的工程师已经暂时的遗忘了自己大学时期学习的数学知识，他们觉得看机器学习的那些公式就像看天书一样。所以他们就坐等那些即将开源的第三方库，想方设法的把它用在自己的业务上，这样也可以吹嘘自己也有了人工智能。\n",
    "\n",
    "### 数学是必要的么\n",
    "数学在机器学习中扮演了重要的角色，但是绝对不是全部，甚至50%都没有。我曾经的一位老师曾经说过一句特别经典的话：我们做的是machine learning，不是math learning。机器学习中的数学远远没有我们想象的复杂，想要在工程中应用他们本来就是一个合格的工程师应该掌握的技能。因为现在的社会太浮躁，钱来的太容易，所以大家就习惯了用重复的劳动来获得稳定的收入。工科的学生一定在本科阶段学习过三门数学基础课，微积分、线性代数和概率统计。机器学习的数学80%就是里面的知识，剩下的20%包括了一些研究生课程如矩阵微积分、数值计算和优化问题，然而其用到的也只是辅助计算的工具，学习的成本并不是很高。\n",
    "\n",
    "### 微积分\n",
    "众所周知，现实世界中其实不存在真正的微分积分，只是差分和求和。微积分在机器学习中的意义在于做一些证明和模型构建，比如在陈天奇开发的xgboost中，他用了二阶泰勒展开代替了原本gbdt中的一阶形式，虽然模型变得复杂了，但是精度提高了。不过说实话，现在gbdt在工业界还是有很多人用作特征选择中，所以还是那句老话，奥卡姆的剃刀原则。\n",
    "\n",
    "微分在机器学习中还有一项应用就是求导，典型的梯度下降和神经网络中的反向传播，都是很简单的实践。现在高中生都能熟练背诵的微分公式，我们就算忘了也该知道到哪里去查吧。\n",
    "\n",
    "微积分的应用仅此而已，说白了就是那些个求导公式。\n",
    "\n",
    "### 线性代数\n",
    "这个东西可以看做现代数学最伟大的发明了，线性代数提供了对高维数据最简便的表示形式，对于工程界和学术界有着非常重大的意义。从我个人的角度看，机器学习理解的坎儿有一半是在这个方面的。为什么这么说呢，因为很多机器学习的公式阐述在数学上看并不是严谨的，有时候它表示的是标量，有时候它表示的矢量甚至是张量。所以在看了前面一大坨推导之后，转念一下前面的条件是不是矢量啊，然后又要从头看一遍，相对而言还是蛮痛苦的。\n",
    "\n",
    "然而，如果因为这个东西过于抽象而放弃它，就是懦夫行径了。先来总结一下机器学习用了哪些线性代数的知识吧：\n",
    "\n",
    "(1) 线性组合。\n",
    "\n",
    "这个东西其实没有必要去翻书本上的定义，线性组合就是字面上理解线性的组合嘛。它在机器学习可以说是无处不在。比如最早接触的线性回归、逻辑回归，基于树的随机森林、提升方法，支持向量机，神经网络，深度学习，都可以看到线性组合的存在。一般在机器学习中，线性组合往往以内积的方式表示。\n",
    "\n",
    "(2) 矩阵的基本运算。\n",
    "\n",
    "这个没什么可说的，加减乘求逆行列式。\n",
    "\n",
    "(3) 特征值和特征向量。\n",
    "\n",
    "这个是重头戏，NG最喜欢拿这个搞事情。网络上对于线性代数最多的问题就是特征值和特征向量怎么理解。然后就有人写了几千字的文章阐述它们的来龙去脉。其实不用这么复杂，特征向量就是一个坐标系里面的一组坐标(不一定正交)，特征值就是这个矩阵(或者说张量)在这组坐标上的值。那特殊一点的就是正交的单位坐标系，比如笛卡尔坐标系。我们知道一个几何坐标都是相对的，因为坐标系不同，那我们怎么知道不同坐标系之间的关系呢。所以后面就有了正交变换和合同变换。\n",
    "\n",
    "在机器学习中，很多地方都有这两者的影子。比如在PCA中，我们需要对协方差矩阵求特征值和特征向量。在谱聚类中，我们需要对Laplace矩阵求特征值和特征向量。就是SVD中的奇异值，其实只是特征值的开方而已。\n",
    "\n",
    "(4) 矩阵微积分。\n",
    "\n",
    "这个是个比较高阶的东西，部分研究生才会有机会上这门课。这个东西和矩阵一样也只是一个工具，是一个为了满足数学的完备性而设计出来的东西。虽然它的定义很复杂，但是我们换一种实数计算的思路去看它，其实很相似。比如矩阵(A的转置 * B)对B求导的结果是啥，答案就是A。如果对A求导呢，答案就是B。那如果二次型(X的转置 * A * X)对X求导结果是啥，答案就是2AX。这个东西就是数学家怕你记不住特地给你设计简单了，既然是工具，我们就不用纠结于它是怎么来的里面的元素究竟怎么分别转置求导了。\n",
    "\n",
    "### 概率统计\n",
    "其实统计和概率其实是两门课，前者研究一些技术指标，后者研究怎么去预测，因为前者的结果对后者有现实意义所以被擅长概括的中国人概括成一门课了。概率统计对于贝叶斯学派有着重大的意义，纵观贝叶斯的学派的各种生成模型推导，p这个字母一定是样本频率最高的字母。\n",
    "\n",
    "首先是统计。\n",
    "\n",
    "我们学了那么统计指标，现在还能记住多少呢。有人说均值，方差，协方差。这就足够了。机器学习中，主要用到的就是均值和方差了。比如PCA中就选择了方差最大的维度作为最优化的目标。再比如多元高斯分布中的先验参数就是均值和方差，我们也往往用样本均值和样本方差去估计他们来达到一些模型的拟合工作。现在搞的风生水起的大数据领域，主要的工作也就是研究怎么在海量数据集上做统计任务。所以，从以前到现在，我们的任务一直都没有变，只是研究的对象变了。\n",
    "\n",
    "其次就是概率了。\n",
    "\n",
    "概率其实只是描述了一种可能性的问题，有个经典的问题，抛硬币正反概率一半对一半，是不是我抛10次，5次正5次反呢。显然不是，那我怎么抛才能真正达到一半对一半呢，答案是无数次。但是现实生活中没有无数次啊，所以有可能我永远不可能抛到一半对一半，所以概率这个东西就是纯理论没有现实意义。这是一种典型的诡辩主义错误，很多人习惯了确定的顺序思维，而忽略了世界的本质其实就是不确定的，量子力学不就是最好的证明么。\n",
    "\n",
    "概率模型是个头大的东西，什么伯努利分布(白努力，就是你努力了也学不会)、二项分布、泊松分布、指数分布、高斯分布，我连名字都记不住别说计算公式了，更别说多元的计算公式了。问题是没有人让你去记公式，以前记是为了考试，现在直接查不就完了。我们需要知道的是他们的特性，什么时候应该套什么样的模型。\n",
    "\n",
    "### 数值计算和优化\n",
    "数值计算，听起来好高深啊，换个俗点的名字就是计算机计算。优化呢，就是求极值。在机器学习的数据，模型准备好的时候，我们就要考虑怎么来求参数的最优解了。\n",
    "\n",
    "大部分情况下，梯度下降可以搞定80%的问题了，当然其中分为BGD(批量)，SGD(随机)和mini batch SGD(小批量随机)。\n",
    "\n",
    "有的时候样本数量特征维数不高，计算资源充足，我们还可以用二阶的牛顿法，去求Hessian矩阵。有时候求Hessian矩阵太占用内存，所有有了后面的各种的拟牛顿的方法(BFGS，LBFGS)。\n",
    "\n",
    "对于隐变量的问题，我们还可以用EM算法来做这件事情，这个方法在无监督学习领域有着重要的意义。\n",
    "\n",
    "当然想SVM中SMO算法，LDA中的Gibbs采样属于小众方法了，可以了解不必执着。\n",
    "\n",
    "### 机器学习工程师应该做什么\n",
    "前面说了，数学50%都不到，剩下的还有什么呢。\n",
    "\n",
    "首先，是对业务的理解。数据脱离了业务就是一潭死水，所以一个数据从业者首先要做的就是结合业务清洗并提炼数据，从而得到需要喂给模型的特征。当然最近出来的深度模型有的可以帮你做到这一点，从而简化了人工特征工程的步骤。\n",
    "\n",
    "其次，是模型选择。当然这个可以是大牛帮你定好方向甚至具体用那个模型，不过大部分时候需要你自己来做决策。所谓技多不压身，除了需要深厚的机器学习的理论功底，也要时刻关注工业学术界新发明的模型来给自己不断的灵感。\n",
    "\n",
    "第三，是调整参数。业内有句行话叫调的一手好参，说明了调参在实际工程中的重要性。这也打破了那些调库党的幻想，当你以为算法api和业务api是一个东西的时候，你突然发现里面的参数你一个都看不懂。这个时候还是得静下心来把算法的整体流程整理一下，了解各个参数的意义，才能更好的为实际的业务服务。\n",
    "\n",
    "最后，就是落地的实现了。码的过程其实大部分时候真的时候调库，除非非常定制化的实现需要自己来做。所以码的大部分工作量主要在于前期数据的预处理，真正的模型部分恐怕不会超过100行(深层的神经网络除外)。\n",
    "\n",
    "### 寄语\n",
    "以此来看，对于理论基础、编码能力和大数据的优先级，个人认为的顺序应该是理论基础＞大数据 > 编码能力。当然有实际的商业化项目肯定会更加加分，但是这并不意味着有了项目可以忽视理论。理论基础可以给你带来源源不断提升潜力，而实际的项目在做到一定程度之后必定还是会演化成重复劳动，所以还是那句老话：路漫漫其修远兮，吾将上下而求索。\n",
    "\n",
    "\n",
    "## Reference\n",
    "\n",
    "[深度学习导论 - 读李宏毅《1天搞懂深度学习》](http://www.cnblogs.com/wdsunny/p/6477018.html)\n",
    "\n",
    "[【286页干货】一天搞懂深度学习（台湾资料科学年会课程）](http://blog.csdn.net/Scythe666/article/details/75446570)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
