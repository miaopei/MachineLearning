{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. TensorFlow 安装\n",
    "\n",
    "## 1.1 OS X 或 Linux\n",
    "\n",
    "运行下列命令来配置开发环境\n",
    "\n",
    "```shell\n",
    "conda create -n tensorflow python=3.6\n",
    "source activate tensorflow\n",
    "conda install pandas matplotlib jupyter notebook scipy scikit-learn\n",
    "conda install -c conda-forge tensorflow\n",
    "```\n",
    "在 Python console 下运行下列代码，检测 TensorFlow 是否正确安装。如果安装正确，Console 会打印出 \"Hello, world!\"。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello World!'\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Create TensorFlow object called tensor\n",
    "hello_constant = tf.constant('Hello World!')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Run the tf.constant operation in the session\n",
    "    output = sess.run(hello_constant)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. TensorFlow 基础\n",
    "\n",
    "## 2.1 Tensor\n",
    "\n",
    "在 TensorFlow 中，数据不是以整数、浮点数或者字符串形式存储的。这些值被封装在一个叫做 tensor 的对象中。在 `hello_constant = tf.constant('Hello World!')` 代码中，`hello_constant` 是一个 0 维度的字符串 tensor，tensor 还有很多不同大小："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A is a 0-dimensional int32 tensor\n",
    "A = tf.constant(1234) \n",
    "# B is a 1-dimensional int32 tensor\n",
    "B = tf.constant([123,456,789]) \n",
    " # C is a 2-dimensional int32 tensor\n",
    "C = tf.constant([ [123,456,789], [222,333,444] ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.constant()` 是你在本课中即将使用的多个 TensorFlow 运算之一。`tf.constant()` 返回的 tensor 是一个常量 tensor，因为这个 tensor 的值不会变。\n",
    "\n",
    "## 2.2 Session\n",
    "TensorFlow 的 api 构建在 computational graph 的概念上，它是一种对数学运算过程进行可视化的方法（在 MiniFlow 这节课中学过）。让我们把你刚才运行的 TensorFlow 代码变成一个图："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://s3.cn-north-1.amazonaws.com.cn/u-img/33f8ba4e-26f9-4f69-8fd6-7e0500fe4117)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如上图所示，一个 \"TensorFlow Session\" 是用来运行图的环境。这个 session 负责分配 GPU(s) 和／或 CPU(s)，包括远程计算机的运算。让我们看看如何使用它："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    output = sess.run(hello_constant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代码已经从之前的一行中创建了 `tensor hello_constant`。接下来是在 session 里对 tensor 求值。\n",
    "\n",
    "这段代码用 `tf.Session` 创建了一个 `sess` 的 session 实例。然后 `sess.run()` 函数对 tensor 求值，并返回结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 输入\n",
    "\n",
    "在上一小节中，你向 session 传入一个 tensor 并返回结果。如果你想使用一个非常量（non-constant）该怎么办？这就是 `tf.placeholder()` 和 `feed_dict` 派上用场的时候了。这一节将向你讲解向 TensorFlow 传输数据的基础知识。\n",
    "\n",
    "**tf.placeholder()**\n",
    "\n",
    "很遗憾，你不能把数据集赋值给 `x` 再将它传给 TensorFlow。因为之后你会想要你的 TensorFlow 模型对不同的数据集采用不同的参数。你需要的是 `tf.placeholder()`！\n",
    "\n",
    "数据经过 `tf.session.run()` 函数得到的值，由 `tf.placeholder()` 返回成一个 tensor，这样你可以在 session 运行之前，设置输入。\n",
    "\n",
    "**Session 的 feed_dict**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.string)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(x, feed_dict={x: 'Hello World'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用 `tf.session.run()` 里的 `feed_dict` 参数设置占位 tensor。上面的例子显示 tensor x 被设置成字符串 \"Hello, world\"。如下所示，也可以用 `feed_dict` 设置多个 tensor。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.string)\n",
    "y = tf.placeholder(tf.int32)\n",
    "z = tf.placeholder(tf.float32)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(x, feed_dict={x: 'Test String', y: 123, z: 45.67})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意：**\n",
    "\n",
    "如果传入 `feed_dict` 的数据与 tensor 类型不符，就无法被正确处理，你会得到 “ValueError: invalid literal for...”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 例子\n",
    "import tensorflow as tf\n",
    "\n",
    "def run():\n",
    "    output = None\n",
    "    x = tf.placeholder(tf.int32)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        # TODO: Feed the x tensor 123\n",
    "        output = sess.run(x, feed_dict={x: 123})\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 TensorFlow 数学\n",
    "\n",
    "获取输入很棒，但是现在你需要使用它。你将使用每个人都懂的基础数学运算，加、减、乘、除，来处理 tensor。（更多数学函数请查看文档）。\n",
    "\n",
    "### 2.4.1 加法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.add(5, 2)  # 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从加法开始，`tf.add()` 函数如你所想，它传入两个数字、两个 tensor、或数字和 tensor 各一个，以 tensor 的形式返回它们的和。\n",
    "\n",
    "### 2.4.2 减法和乘法\n",
    "\n",
    "这是减法和乘法的例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.subtract(10, 4) # 6\n",
    "y = tf.multiply(2, 5)  # 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`x` tensor 求值结果是 `6`，因为 `10 - 4 = 6`。`y` tensor 求值结果是 `10`，因为 `2 * 5 = 10`。是不是很简单！\n",
    "\n",
    "### 2.4.3 类型转换\n",
    "\n",
    "为了让特定运算能运行，有时会对类型进行转换。例如，你尝试下列代码，会报错："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tf.subtract(tf.constant(2.0),tf.constant(1))  \n",
    "# Fails with ValueError: Tensor conversion requested dtype float32 for Tensor with dtype int32:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这是因为常量 `1` 是整数，但是常量 `2.0` 是浮点数，`subtract` 需要它们的类型匹配。\n",
    "\n",
    "在这种情况下，你可以确保数据都是同一类型，或者强制转换一个值为另一个类型。这里，我们可以把 `2.0` 转换成整数再相减，这样就能得出正确的结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Sub_2:0' shape=() dtype=int32>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.subtract(tf.cast(tf.constant(2.0), tf.int32), tf.constant(1))   # 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n"
     ]
    }
   ],
   "source": [
    "# 例子\n",
    "import tensorflow as tf\n",
    "\n",
    "# TODO: Convert the following to TensorFlow:\n",
    "x = tf.constant(10)\n",
    "y = tf.constant(2)\n",
    "z = tf.subtract(tf.divide(x,y), tf.cast(tf.constant(1), tf.float64))\n",
    "\n",
    "# TODO: Print z from a session\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(z)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**总结：**\n",
    "\n",
    "* 在 `tf.Session` 里面进行运算。\n",
    "* 通过 `tf.constant()` 创建常量 tensor。\n",
    "* 用 `tf.placeholder()` 和 `feed_dict` 得到输入。\n",
    "* 应用 `tf.add()`、 `tf.subtract()`、 `tf.multiply()` 和 `tf.divide()` 函数进行数学运算。\n",
    "* 学习如何用 `tf.cast()` 进行类型转换。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. TensorFlow 里的线性函数\n",
    "神经网络中最常见的运算，就是计算输入、权重和偏差的线性组合。回忆一下，我们可以把线性运算的输出写成：\n",
    "\n",
    "![](https://s3.cn-north-1.amazonaws.com.cn/u-img/d264f44e-8d50-459a-8664-3a4499772afa)\n",
    "\n",
    "这里 $\\mathbf{W}$ 是连接两层的权重矩阵。输出 $\\mathbf{y}$，输入 $\\mathbf{x}$，偏差 $\\mathbf{b}$ 全部都是向量。\n",
    "\n",
    "## 3.1 TensorFlow 里的权重和偏差\n",
    "\n",
    "训练神经网络的目的是更新权重和偏差来更好地预测目标。为了使用权重和偏差，你需要一个能修改的 Tensor。这就排除了 `tf.placeholder()` 和 `tf.constant()`，因为它们的 Tensor 不能改变。这里就需要 `tf.Variable` 了。\n",
    "\n",
    "**tf.Variable()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.Variable(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.Variable` 类创建一个 tensor，其初始值可以被改变，就像普通的 Python 变量一样。该 tensor 把它的状态存在 session 里，所以你必须手动初始化它的状态。你将使用 `tf.global_variables_initializer()` 函数来初始化所有可变 tensor。\n",
    "\n",
    "**初始化**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.global_variables_initializer()` 会返回一个操作，它会从 graph 中初始化所有的 TensorFlow 变量。你可以通过 session 来调用这个操作来初始化所有上面的变量。用 `tf.Variable` 类可以让我们改变权重和偏差，但还是要选择一个初始值。\n",
    "\n",
    "从正态分布中取随机数来初始化权重是个好习惯。随机化权重可以避免模型每次训练时候卡在同一个地方。在下节学习梯度下降的时候，你将了解更多相关内容。\n",
    "\n",
    "类似地，从正态分布中选择权重可以避免任意一个权重与其他权重相比有压倒性的特性。你可以用 `tf.truncated_normal()` 函数从一个正态分布中生成随机数。\n",
    "\n",
    "**tf.truncated_normal()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_features = 120\n",
    "n_labels = 5\n",
    "weights = tf.Variable(tf.truncated_normal((n_features, n_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.truncated_normal()` 返回一个 tensor，它的随机值取自一个正态分布，并且它们的取值会在这个正态分布平均值的两个标准差之内。\n",
    "\n",
    "因为权重已经被随机化来帮助模型不被卡住，你不需要再把偏差随机化了。让我们简单地把偏差设为 0。\n",
    "\n",
    "**tf.zeros()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_labels = 5\n",
    "bias = tf.Variable(tf.zeros(n_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.zeros()` 函数返回一个都是 0 的 tensor。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. ReLU 和 Softmax 激活函数\n",
    "\n",
    "之前，我们使用 S 型函数作为隐藏单元上的激活函数，对于分类来说，则是输出单元上的激活函数。但是，S 型函数并非是唯一可以使用的激活函数，实际上它具有一些不足。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://s3.cn-north-1.amazonaws.com.cn/u-img/473d8fff-eb0d-4262-b7ea-0356252a4dcb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正如在反向传播资料中提到的，S 型函数的导数最大值为 0.25（如上所示）。这意味着，当你用 S 型函数单元进行反向传播时，网络上每层出现的错误至少减少 75%，如果有很多层，权重更新将很小，这些权重需要很长的训练时间。因此，S 型函数不适合作为隐藏单元上的激活函数。\n",
    "\n",
    "## 4.1 初识修正线性单元（ReLU）\n",
    "\n",
    "近期的大多数深度学习网络都对隐藏层使用**修正线性单元 (ReLU)**，而不是 S 型函数。如果输入小于 0，修正线性单元的输出是 0，原始输出则相反。即如果输入大于 0，则输出等于输入。从数学角度来看，如下所示：\n",
    "\n",
    "$f(x) = \\mathrm{max}(x, 0)$\n",
    "\n",
    "函数的输出是输入值 $x$ 或 $0$（取较大者）。所以，如果 $x = -1$，那么 $f(x) = 0$；如果 $x =0.5$，那么 $f(x) = 0.5$。图表如下所示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://s3.cn-north-1.amazonaws.com.cn/u-img/1e33c195-9796-4d18-8752-bc956f5ddc10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU 激活函数是你可以使用的最简单非线性激活函数。当输入是正数时，导数是 1，所以没有 S 型函数的反向传播错误导致的消失效果。研究表明，对于大型神经网络来说，ReLU 的训练速度要快很多。TensorFlow 和 TFLearn 等大部分框架使你能够轻松地在隐藏层使用 ReLU，你不需要自己去实现这些 ReLU。\n",
    "\n",
    "**不足**\n",
    "\n",
    "有时候一个非常大的梯度流过一个 ReLU 神经元，更新过参数之后，会使ReLU神经元始终为 0（这个神经元再也不会对任何数据有激活现象了）。这些“无效”的神经元将始终为 0，很多计算在训练中被浪费了。\n",
    "\n",
    "摘自 Andrej Karpathy 的 CS231n 课程:\n",
    "\n",
    "> 遗憾的是，ReLU 单元在训练期间可能会很脆弱并且会变得“无效”。例如，流经 ReLU 神经元的大型梯度可能会导致权重按以下方式更新：神经元将再也不会在任何数据点上激活。如果发生这种情况，那么流经该单元的梯度将自此始终为零。也就是说，ReLU 单元会在训练期间变得无效并且不可逆转，因为它们可能会不再位于数据流形上。例如，学习速度（learning rate）设置的太高，你的网络可能有高达 40% 的神经元处于“无效”状态（即神经元在整个训练数据集上从未激活）。如果能正确地设置学习速度，那么该问题就不太容易出现。\n",
    "\n",
    "**Softmax**\n",
    "\n",
    "之前，我们看到神经网络用在了回归（骑车用户问题）和二元分类（研究生招生问题）中。通常，你会希望预测某个输入是否处于很多类别中的某一个类别。这是一种分类问题，但是 S 型函数不再是最佳选择。我们将会使用 [softmax](https://en.wikipedia.org/wiki/Softmax_function) 函数。和 sigmoid 一样，softmax 函数将每个单元的输出压缩到 0 和 1 之间。但 softmax 函数在拆分输出时，会使输出之和等于 1。softmax 函数的输出等于分类概率分布，显示了任何类别为真的概率。\n",
    "\n",
    "softmax 函数与普通 sigmoid 之间的真正差别是 softmax 会标准化输出，使输出之和等于 1。对于这两种函数，你都可以输入向量，并获得输出为相同大小的向量，但是所有值都压缩在 0 和 1 之间。sigmoid 可用于只有一个输出单元的二元分类。但是如果进行多项分类的话，则需要使用多个输出单元（每个类别一个单元），并对输出进行 softmax 激活。\n",
    "\n",
    "例如，softmax 函数有三个输入，那么网络有三个输出单元，则结果如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://s3.cn-north-1.amazonaws.com.cn/u-img/6ea53f11-b47e-4508-9172-c853d8f60574)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "softmax 函数的数学表达式如下所示，其中 $\\mathbf{z}$ 是输出层的输入向量（如果你有 10 个输出单元，则 $\\mathbf{z}$ 中有 10 个元素）。同样，$j$ 表示输出单元的索引。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://s3.cn-north-1.amazonaws.com.cn/u-img/ffb9dc37-c2e9-458a-98da-fd799b186a66)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看上去的确很难懂，但实际上很简单，如果不懂数学原理的话，也没关系。只需记住输出经过压缩，和为 1。\n",
    "\n",
    "softmax 可以用于任何数量的分类。接下来你将看到，它可以用于预测两种类别的情感（积极和消极）。还可以用于成百上千的物体分类，例如物体识别问题中，需要识别数以百计不同种类的物体。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
