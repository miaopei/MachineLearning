{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://s3.cn-north-1.amazonaws.com.cn/u-img/07e338ce-41fa-4b2a-b1b9-5997261c3f58)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 感知器 Perceptron\n",
    "\n",
    "现在你看到了一个简单的神经网络如何做决策：输入数据，处理信息，然后给出一个结果作为输出！现在让我们深入这个大学录取的案例，来学习更多输入数据如何被处理。\n",
    "\n",
    "数据，无论是考试成绩还是评级，被输入到一个相互连接的节点网络中。这些独立的节点被称作 [感知器](https://en.wikipedia.org/wiki/Perceptron) 或者神经元。它们是构成神经网络的基本单元。每个感知器依照输入数据来决定如何对数据分类。 在上面的例子中，输入的评级或者成绩要么通过阈值 (threshold) 要么通不过。这些分类组合形成一个决策 - 例如，如果两个节点都返回 “yes“，这个学生就被学校录取了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://s3.cn-north-1.amazonaws.com.cn/u-img/56c2f101-2c07-4e1d-bf3a-f5efdf1f889a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们进一步放大来看一个单个感知器如何处理输入数据。\n",
    "\n",
    "上图中的感知器是视频中决定学生是否被录取的两个感知器之一。它决定了学生的评级是否应该被学校录取。你也许会问：“它怎么知道在做录取决定的时候是分数更重要还是评级更重要？”。事实上，当我们初始化神经网络的时候，不知道哪个信息对决定更重要。这需要神经网络 自己学习 出哪个数据更重要，然后调整。\n",
    "\n",
    "神经网络通过一个叫做 **Weight（权重）**的东西来做这件事。\n",
    "\n",
    "## 1.1 权重\n",
    "\n",
    "当数据被输入感知器，它会与分配给这个特定输入的权重相乘。例如，上图感知器有两个输入，`tests` 和 `grades`，所以它有两个与之相关的权重，并且可以分别调整。这些权重刚开始是随机值，当神经网络学习到什么样的输入数据会使得学生被学校录取之后，网络会根据之前权重下分类的错误来调整权重，这个过程被称为神经网络的 **训练**。\n",
    "\n",
    "一个较大的权重意味着神经网络认为这个输入比其它输入更重要，较小的权重意味着数据不是那么重要。一个极端的例子是，如果 test 成绩对学生录取没有影响，那么 test 分数的权重就会是零，也就是说，它对感知器的输出没有影响。\n",
    "\n",
    "## 1.2 输入数据加总\n",
    "所以，每个感知器的输入需要有一个关联的权重代表它的重要性，这个权重是由神经网络的学习过程决定的，也就是训练。接下来，经过加权的输入数据被加总，生成一个单独的值，它会帮助实现最终输出 - 也就是这个学生是否被录取。让我们看一个实际的例子："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://s3.cn-north-1.amazonaws.com.cn/u-img/4283bc6e-b7de-4ac4-8130-22f43a32c9cd)\n",
    "\n",
    "**我们把 `x_test` 跟它的权重 `w_test` 相乘的结果与 `x_grades` 与 `w_grades` 相乘的结果相加。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在写跟神经网络有关的公式的时候，权重总是被各种样式的字母 $w$ 来表示。$W$ 通常表示权重矩阵，而 $\\omega$ 被用来表示单个权重。有时还会有些额外的信息以下标的形式给出（后面会见到更多）。记住你看到字母 $w$，想到**权重**是不会错的。\n",
    "\n",
    "在这个例子中，我们用 $w_{grades}$ 来表示 `grades` 的权重，$w_{test}$ 来表示 `test` 的权重。在上图中，假设权重是 $w_{grades} = -1$, $w_{test}\\ = -0.2$ 你不用关注它们具体的值，它们的相对值更重要。 $w_{grades}$ 是 $w_{test}$ 的 5 倍，代表神经网络认为在判断学生是否能被大学录取时， `grades` 的重要性是 `test` 的 5 倍。\n",
    "\n",
    "感知器把权重应用于输入再加总的过程叫做 **线性组合**。在这里也就是\n",
    "\n",
    "$w_{grades} \\cdot x_{grades} + w_{test} \\cdot x_{text} = -1 \\cdot x_{grades} - 0.2 \\cdot x_{test} $\n",
    "\n",
    "为了让我们的公式更简洁，我们可以把名称用数字表示。用 11 来表示 gradesgrades，22 来表示 teststests。我们的公式就变成了：\n",
    "\n",
    "$w_{1} \\cdot x_{1} + w_{2} \\cdot x_{2}$\n",
    "\n",
    "在这个例子中，我们只有两个简单的输入，grades 和 tests。试想如果我们有 m 个不同的输入可以把他们标记成 $x_1, x_2, ..., x_m$ 。对应 $x_1$ 的权重是 $w_1$ 以此类推。在这里，我们的线性组合可以简洁得写成：\n",
    "\n",
    "$\\sum^{m}_{i=1} w_i \\cdot x_i$\n",
    "\n",
    "这里，希腊字母 Sigma $\\sum$ 用来表示 **求和**。它意思是求解其右边表达式，并把所有结果加总。也就是说，这里求的是所有 $w_i \\cdot x_i$ 的和。\n",
    "\n",
    "但是我们这里从哪里找到 $w_i$ 和 $x_i$ ？\n",
    "\n",
    "$\\sum^{m}_{i=1}$ 意思是遍历所有的 $i$ 值，从 1 到 $m$。\n",
    "\n",
    "这些都组合在一起 $\\sum^{m}_{i=1} w_i \\cdot x_i$ 表示：\n",
    "\n",
    "* 以 $i = 1$ 开始\n",
    "* 求 $w_i \\cdot x_1$ 并记住结果\n",
    "* 让 $i = 2$\n",
    "* 求 $w_2 \\codt x_2$ 并记住结果\n",
    "* 重复这个过程直到 $i = m$，其中 $m$ 是输入的个数\n",
    "\n",
    "最后，无论是在这里还是你自己的阅读中，你都会看到公式有很多种写法。例如：你会看到$ \\sum_{i}$ 而不是 $\\sum_{i=1} ^ m$。第一个只是第二个的简写。也就是说你看到一个没有起始点和截止点的求和，就表示要把它们全部加起来。 有时候，如果遍历的值可以被推断，你可能会看到一个单独的 $\\sum$。记住，它们都是相同的：\n",
    "\n",
    "$\\sum^{m}_{i=1}w_i \\cdot x_i = \\sum_{i}w_i \\cdot x_i = \\sum w_i \\cdot x_i$\n",
    "\n",
    "## 1.3 计算激活函数的输出\n",
    "\n",
    "最后，感知器求和的结果会被转换成输出信号，这是通过把线性组合传给 **激活函数** 来实现的。\n",
    "\n",
    "当输入给到节点，激活函数可以决定节点的输出。因为它决定了实际输出，我们也把层的输出，称作“激活”。\n",
    "\n",
    "最简单的激活函数之一是 **单位阶跃函数（Heaviside step function）**。如果线性组合小于 0，函数返回 0，如果线性组合等于或者大于 0，函数返回 1。 [单位阶跃函数（Heaviside step function）](https://en.wikipedia.org/wiki/Heaviside_step_function) 如下图，其中 h 是已计算的线性组合："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://s3.cn-north-1.amazonaws.com.cn/u-img/e5879d46-2dc8-4f23-af5a-e7047c46c277)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://s3.cn-north-1.amazonaws.com.cn/u-img/663724e2-77eb-4b2c-ba32-5ec0663fb725)\n",
    "\n",
    "**单位阶跃函数 Heaviside step function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上面这个大学录取的例子中，我们用了 $w_{grades} = -1, w_{test}\\ = -0.2$ 作为权重。因为  $w_{grades}$ 和 $w_{test}$ 都是负值，激活函数只有在 grades 和 test 都是 0 的时候返回 1。这是由于用这些权重和输入做线性组合的取值范围是 $(-\\infty, 0]$ （负无穷到 0，包括 0）。\n",
    "\n",
    "用二维数据举例看起来最容易。在下图中，想象线上以及阴影部分的任何一点，代表所有可能对节点的输入。同时，想象 $y$ 轴上的值为输入与适当权重的线性组合结果。并将这个结果作为激活函数的输入。\n",
    "\n",
    "记得我们说过，单位阶跃函数对任何大于等于 0 的输入，都返回 $1$ 像你在图中看到的，只有一个点的 $y$ 值大于等于 $0$： 就是 (0, 0)(0,0)原点："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://s3.cn-north-1.amazonaws.com.cn/u-img/12527ca8-0308-4ebd-b6db-180df465770d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当然，我们想要更多可能的 grade/test 组合在录取组里，所以我们需要对我们的激活函数做调整使得它对更多的输入返回 $1$，特别是，我们要找到一种办法，让所有我们希望录取的人输入和权重的线性组合的值大于等于 $0$。\n",
    "\n",
    "使我们函数返回更多 $1$ 的一种方式是往我们线性组合的结果里加上一个 **偏置项（bias）**。\n",
    "\n",
    "偏置项在公式中用 $b$ 来表示，让我们移动一下各个方向上的值。\n",
    "\n",
    "例如，下图蓝色部分代表先前的假设函数加了 $+3$ 的偏置项。蓝色阴影部分表示所有激活的值。注意，这个结果的输入，跟之前灰色部分的输入是一样的，只是加了偏置项之后，它被调整的更高了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://s3.cn-north-1.amazonaws.com.cn/u-img/5d66a912-6a6c-47bf-91f8-af124cea0cde)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，我们并不能事先知道神经网络该如何选择偏置项。但没关系，偏置项跟权重一样，可以在训练神经网络的时候更新和改变。增加了偏置项之后，我们有了一个完整的感知器公式："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://s3.cn-north-1.amazonaws.com.cn/u-img/2f8a0434-cb9c-4395-b7e4-bd5e993d0aeb)\n",
    "\n",
    "**感知器公式**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果输入 $(x_1, x_2, ..., x_m)$ 属于被录取的学生，公式返回 $1$ 如果输入 $(x_1, x_2, ..., x_m)$ 属于未被录取的学生，公式返回 $0$。输入由一个或多个实数组成，每一个由 $x_i$ 代表，$m$ 则代表总共有多少个输入。\n",
    "\n",
    "然后神经网络开始学习！权重 $(w_i)$ 和偏置项 $(b)$ 被初始化为一个随机值，然后神经网络使用一种学习算法（比如梯度下降算法）来更新它们的值。权重和偏置项的更新使得下一个训练样本更准确地被归类，数据中蕴含的模式，也就被神经网络“学”出来了。\n",
    "\n",
    "现在你对感知器有了很好的理解，让我们把学到的知识予以应用。接下来，你将通过设定权重和偏置项来创建一个之前神经网络的视频中学到的 AND 感知器。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. 最简单的神经网络\n",
    "\n",
    "目前为止，我们接触的感知器的输出非 0 即 1，输出单元的输入经过了一个激活函数 f(h)f(h) 在此处就是指阶跃函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://s3.cn-north-1.amazonaws.com.cn/u-img/a471a490-8efa-425c-8f1c-c5e329721c3d)\n",
    "\n",
    "![](https://s3.cn-north-1.amazonaws.com.cn/u-img/28d3c389-eef8-44bd-b435-4a833c1a9d2c)\n",
    "\n",
    "**阶跃激活函数**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出单元返回的是 $f(h)$ 的结果，其中 $h$ 是输出单元的输入：\n",
    "\n",
    "$h = \\sum_i w_i x_i + b$\n",
    "\n",
    "下图展示了一个简单的神经网络。权重、输入和偏置项的线性组合构成了输入 $h$，其通过激活函数 $f(h)$，给出感知器最终的输出，标记为 $y$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://s3.cn-north-1.amazonaws.com.cn/u-img/e429472f-a8bf-411a-87e5-6abf1223a725)\n",
    "\n",
    "**神经网络示意图，圆圈代表单元，方块是运算**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个架构最酷的一点，也是使得神经网络可以实现的原因，就是激活函数 $f(h)$ 可以是 **任何函数**，并不只是上面提到的阶跃函数。\n",
    "\n",
    "例如，如果让 $f(h) = h$，输出等于输入，那网络的输出就是：\n",
    "\n",
    "$y = \\sum_iw_ix_i + b$\n",
    "\n",
    "你应该非常熟悉这个公式，它跟线性回归模型是一样的！\n",
    "\n",
    "其它常见激活函数还有对数几率（又称作 sigmoid），tanh 和 softmax。这节课中我们主要使用 sigmoid 函数：\n",
    "\n",
    "$\\mathrm{sigmoid}(x) = 1/(1+e^{-x})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://s3.cn-north-1.amazonaws.com.cn/u-img/e43896bc-5796-4c40-8312-f162863b142e)\n",
    "\n",
    "**sigmoid 函数**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sigmoid 函数值域是 0 到 1 之间，它的输出还可以被解释为成功的概率。实际上，用 sigmoid 函数作为激活函数的结果，跟对数几率回归是一样的。\n",
    "\n",
    "这就是感知器到神经网络的改变，在这个简单的网络中，跟通常的线性模型例如对数几率模型相比，神经网络还没有展现出任何优势。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://s3.cn-north-1.amazonaws.com.cn/u-img/41fd77b9-559c-4baf-a7c1-7d3bc2f25efd)\n",
    "\n",
    "**如你之前所见，在 XOR 感知器中，把感知器组合起来可以让我们对线性不可分的数据建模。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但是，如你所见，在 XOR 感知器中，虽然把感知器组合起来可以对线性不可分的数据建模，但是却无法对回归模型建模。\n",
    "\n",
    "你一旦开始用连续且可导的激活函数后，就能够运用梯度下降来训练网络，这就是你接下来将要学到的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. 梯度下降\n",
    "\n",
    "## 3.1 学习权重\n",
    "\n",
    "你了解了如何使用感知器来构建 AND 和 XOR 运算，但它们的权重都是人为设定的。如果你要进行一个运算，例如预测大学录取结果，但你不知道正确的权重是什么，该怎么办？你要从样本中学习权重，然后用这些权重来做预测。\n",
    "\n",
    "要了解我们将如何找到这些权重，可以从我们的目标开始考虑。我们想让网络做出的预测与真实值尽可能接近。为了能够衡量，我们需要有一个指标来了解预测有多差，也就是 **误差 (error)**。一个普遍的指标是误差平方和 sum of the squared errors (SSE)：\n",
    "\n",
    "$E = \\frac{1}{2}\\sum_\\mu \\sum_j \\left[ y^{\\mu}_j - \\hat{y} ^{\\mu} _j \\right]^2$\n",
    "\n",
    "这里 $\\hat y$ 是预测值 $y$ 是真实值。一个是所有输出单元 $j$ 的和，另一个是所有数据点 $\\mu$ 的和。这里看上去很复杂，但你一旦理解了这些符号之后，你就能明白这是怎么回事了。\n",
    "\n",
    "首先是内部这个对 $j$ 的求和。变量 $j$ 代表网络输出单元。所以这个内部的求和是指对于每一个输出单元，计算预测值 $\\hat y$ 与真实值 $y$ 之间的差的平方，再求和。\n",
    "\n",
    "另一个对 $\\mu$ 的求和是针对所有的数据点。也就是说，对每一个数据点，计算其对应输出单元的方差和，然后把每个数据点的方差和加在一起。这就是你整个输出的总误差。\n",
    "\n",
    "SSE 是一个很好的选择有几个原因：误差的平方总是正的，对大误差的惩罚大于小误差。同时，它对数学运算也更友好。\n",
    "\n",
    "回想神经网络的输出，也就是预测值，取决于权重\n",
    "\n",
    "$\\hat{y}^{\\mu}_j = f \\left( \\sum_i{ w_{ij} x^{\\mu}_i }\\right)$ \n",
    "\n",
    "相应的，误差也取决于权重\n",
    "\n",
    "$E = \\frac{1}{2}\\sum_\\mu \\sum_j \\left[ y^{\\mu}_j - f \\left( \\sum_i{ w_{ij} x^{\\mu}_i }\\right) \\right]^2$\n",
    "\n",
    "我们想让网络预测的误差尽可能小，权重是让我们能够实现这个目标的调节旋钮。我们的目的是寻找权重 $w_{ij}$ 使得误差平方 $E$ 最小。通常来说神经网络通过**梯度下降**来实现这一点。\n",
    "\n",
    "## 3.2 梯度下降\n",
    "\n",
    "用梯度下降，我们通过多个小步骤来实现目标。在这个例子中，我们希望一步一步改变权重来减小误差。借用前面的比喻，误差就像是山，我们希望走到山下。下山最快的路应该是最陡峭的那个方向，因此我们也应该寻找能够使误差最小化的方向。我们可以通过计算误差平方的梯度来找到这个方向。\n",
    "\n",
    "**梯度**是改变率或者斜度的另一个称呼。如果你需要回顾这个概念，可以看下可汗学院对这个问题的[讲解](https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/gradient-and-directional-derivatives/v/gradient)。\n",
    "\n",
    "要计算变化率，我们要转向微积分，具体来说是导数。一个函数 $f(x)$ 的导函数 $f'(x)$ 给到你的是 $f(x)$ 在 $x$ 这一点的斜率。例如 $x^2$，$x^2$ 的导数是 $f'(x) = 2x$。所以，在 $x = 2$ 这个点斜率 $f'(2) = 4$。画出图来就是："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://s3.cn-north-1.amazonaws.com.cn/u-img/1fce3562-a676-4a58-81c4-772db2ed6a5d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "梯度就是对多变量函数导数的泛化。我们可以用微积分来寻找误差函数中任意一点的梯度，它与输入权重有关，下一节你可以看到如何推导梯度下降的步骤。\n",
    "\n",
    "下面我画了一个拥有两个输入的神经网络误差示例，相应的，它有两个权重。你可以将其看成一个地形图，同一条线代表相同的误差，较深的线对应较大的误差。\n",
    "\n",
    "每一步，你计算误差和梯度，然后用它们来决定如何改变权重。重复这个过程直到你最终找到接近误差函数最小值的权重，即中间的黑点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://s3.cn-north-1.amazonaws.com.cn/u-img/bf8d1191-6d28-4940-9625-b033ca0e5720)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 注意事项\n",
    "\n",
    "因为权重会走向梯度带它去的位置，它们有可能停留在误差小，但不是最小的地方。这个点被称作局部最低点。如果权重初始值有错，梯度下降可能会使得权重陷入局部最优，例如下图所示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://s3.cn-north-1.amazonaws.com.cn/u-img/53b0f168-9dce-4a56-9601-2a965e171392)\n",
    "**梯度下降引向局部最低点**\n",
    "\n",
    "有方法可以避免这一点，被称作 [momentum](http://sebastianruder.com/optimizing-gradient-descent/index.html#momentum)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 梯度下降：数学\n",
    "之前我们看到一个权重的更新可以这样计算：\n",
    "\n",
    "$\\Delta w_i = \\eta \\, \\delta x_i$\n",
    "\n",
    "这里 error term $\\delta$ 是指\n",
    "\n",
    "$\\delta = (y - \\hat y) f'(h) = (y - \\hat y) f'(\\sum w_i x_i)$\n",
    "\n",
    "记住，上面公式中 $(y - \\hat y)$ 是输出误差，激活函数 $f(h)$ 的导函数是 $f'(h)$，我们把这个导函数称做输出的梯度。\n",
    "\n",
    "### 3.3.1 均方差\n",
    "\n",
    "这里我们要对如何计算误差做一点小改变。我们不计算 SSE，而是用误差平方的**均值**（mean of the square errors，MSE）。现在我们要处理很多数据，把所有权重更新加起来会导致很大的更新，使得梯度下降无法收敛。为了避免这种情况，你需要一个很小的学习率。这里我们还可以除以数据点的数量 mm 来取平均。这样，无论我们有多少数据，我们的学习率通常会在 0.01 to 0.001 之间。我们用 MSE（下图）来计算梯度，结果跟之前一样，只是取了平均而不是取和。\n",
    "\n",
    "![](https://s3.cn-north-1.amazonaws.com.cn/u-img/3da8626b-9845-4882-a560-ce1bd8c0618d)\n",
    "\n",
    "这是用梯度下降来更新权重的算法概述：\n",
    "\n",
    "* 权重步长设定为 0： $\\Delta w_i = 0$\n",
    "* 对训练数据中的每一条记录：\n",
    "    * 通过网络做正向传播，计算输出 $\\hat y = f(\\sum_i w_i x_i)$ \n",
    "    * 计算输出单元的误差项（error term） $\\delta = (y - \\hat y) * f'(\\sum_i w_i x_i)$\n",
    "    * 更新权重步长 $\\Delta w_i = \\Delta w_i + \\delta x_i$\n",
    "* 更新权重 $w_i = w_i + \\eta \\Delta w_i / m$。其中 $\\eta$ 是学习率， $m$ 是数据点个数。这里我们对权重步长做了平均，为的是降低训练数据中大的变化。\n",
    "* 重复 $e$ 代（epoch）。\n",
    "\n",
    "你也可以对每条记录更新权重，而不是把所有记录都训练过之后再取平均。\n",
    "\n",
    "这里我们还是使用 sigmoid 作为激活函数\n",
    "\n",
    "$f(h) = 1/(1+e^{-h})$\n",
    "\n",
    "sigmoid 的梯度是： $f'(h) = f(h) (1 - f(h))$\n",
    "\n",
    "其中 $h$ 是输出单元的输入\n",
    "\n",
    "$h = \\sum_i w_i x_i$\n",
    "\n",
    "### 3.3.2 用 NumPy 来实现\n",
    "\n",
    "这里大部分都可以用 NumPy 很方便的实现。\n",
    "\n",
    "首先你需要初始化权重。我们希望它们比较小，这样输入在 sigmoid 函数那里可以在接近 0 的位置，而不是最高或者最低处。很重要的一点是要随机地初始化它们，这样它们有不同的初始值，是发散且不对称的。所以我们用一个中心为 0 的正态分布来初始化权重，此正态分布的标准差（scale 参数）最好使用 $1/\\sqrt{n}$，其中 $n$ 是输入单元的个数。这样就算是输入单元的数量变多，sigmoid 的输入还能保持比较小。\n",
    "\n",
    "```python\n",
    "weights = np.random.normal(scale=1/n_features**.5, size=n_features)\n",
    "```\n",
    "\n",
    "NumPy 提供了一个可以让两个数组做点乘的函数，它可以让我们方便地计算 $h$。点乘就是把两个数组的元素对应位置相乘之后再相加。\n",
    "\n",
    "```python\n",
    "# input to the output layer\n",
    "# 输出层的输入\n",
    "output_in = np.dot(weights, inputs)\n",
    "```\n",
    "\n",
    "最后我们可以用 weights += ... 更新 $\\Delta w_i$ 和 $w_i$，weights += ... 是 weights = weights + ... 的简写。\n",
    "\n",
    "### 3.3.3 效率提示\n",
    "\n",
    "因为这里我们用的是 sigmoid 函数，你可以节省一些计算。对于 sigmoid 函数来说，$f'(h) = f(h) (1 - f(h))$。也就是说一旦你有了 $f(h)$，你就可以直接用它的值来计算误差的梯度了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. 多层感知器\n",
    "\n",
    "之前我们研究的是只有一个输出节点网络，代码也很直观。但是现在我们有多个输入单元和多个隐藏单元，它们的权重需要有两个索引 $w_{ij}$，其中 $i$ 表示输入单元，$j$ 表示隐藏单元。\n",
    "\n",
    "例如在下面这个网络图中，输入单元被标注为 $x_1, x_2,, x_3$，隐藏层节点是 $h_1$ 和 $h_2$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://s3.cn-north-1.amazonaws.com.cn/u-img/c22ee73f-ff58-47da-a248-e319cbb046d2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代表指向 $h_1$ 的权重的线条被标成了红色，这样更好区分。\n",
    "\n",
    "为了定位权重，我们把输入节点的索引 $i$ 和隐藏节点的索引 $j$ 结合，得到：\n",
    "\n",
    "$w_{11}$\n",
    "\n",
    "代表从 $x_1$ 到 $h_1$ 的权重；\n",
    "\n",
    "$w_{12}$ \n",
    "\n",
    "代表从 $x_1$ 到 $h_2$ 的权重。\n",
    "\n",
    "下图包括了从输入层到隐藏层的所有权重，用 $w_{ij}$ 表示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://s3.cn-north-1.amazonaws.com.cn/u-img/ee96afdd-a46e-4be6-b6b2-39fc9c8ff2b5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "之前我们可以把权重写成数组，用 $w_i$ 来索引。\n",
    "\n",
    "现在，权重被储存在 **矩阵** 中，由 $w_{ij}$ 来索引。矩阵中的每一**行**对应从同一个**输入**节点**发出**的权重，每一**列**对应**传入**同一个**隐藏**节点的权重。这里我们有三个输入节点，两个隐藏节点，权重矩阵表示为："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://s3.cn-north-1.amazonaws.com.cn/u-img/6f15956b-0cf5-4c07-a7b0-db1c8db26653)\n",
    "\n",
    "**三个输入节点两个隐藏节点的权重矩阵**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对比上面的示意图，确保你了解了不同的权重在矩阵中与在神经网络中的对应关系。\n",
    "\n",
    "用 NumPy 来初始化这些权重，我们需要提供矩阵的形状（shape），如果特征是一个包含以下数据的二维数组：\n",
    "\n",
    "```python\n",
    "# Number of records and input units\n",
    "# 数据点数量以及每个数据点有多少输入节点\n",
    "n_records, n_inputs = features.shape\n",
    "# Number of hidden units\n",
    "# 隐藏层节点个数\n",
    "n_hidden = 2\n",
    "weights_input_to_hidden = np.random.normal(0, n_inputs**-0.5, size=(n_inputs, n_hidden))\n",
    "```\n",
    "\n",
    "这样创建了一个名为 weights_input_to_hidden 的二维数组，维度是 n_inputs 乘 n_hidden。记住，输入到隐藏节点是所有输入乘以隐藏节点权重的和。所以对每一个隐藏层节点 $h_j$，我们需要计算：\n",
    "\n",
    "![](https://s3.cn-north-1.amazonaws.com.cn/u-img/74bfbba1-1195-4e67-b3ce-0ed3e3f78f38)\n",
    "\n",
    "为了实现这点，我们需要运用 [矩阵乘法](https://en.wikipedia.org/wiki/Matrix_multiplication)，如果你对线性代数的知识有些模糊，我们建议你看下之前先修部分的资料。这里你只需要了解矩阵与向量如何相乘。\n",
    "\n",
    "在这里，我们把输入（一个行向量）与权重相乘。要实现这个，要把输入点乘（内积）以权重矩阵的每一列。例如要计算到第一个隐藏节点 $j = 1$ 的输入，你需要把这个输入与权重矩阵的第一列做点乘："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://s3.cn-north-1.amazonaws.com.cn/u-img/82fcbaec-580b-42d1-b862-f18b3eae3c14)\n",
    "\n",
    "**用输入与权重矩阵的第一列相乘得出到隐藏层第一个节点的输入**\n",
    "\n",
    "![](https://s3.cn-north-1.amazonaws.com.cn/u-img/94726d0f-6454-44b5-bb3c-6611d1f60966)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "针对第二个隐藏节点的输入，你需要计算输入与第二列的点积，以此类推。\n",
    "\n",
    "在 NumPy 中，你可以直接使用 np.dot\n",
    "\n",
    "```python\n",
    "hidden_inputs = np.dot(inputs, weights_input_to_hidden)\n",
    "```\n",
    "\n",
    "你可以定义你的权重矩阵的维度是 n_hidden 乘 n_inputs 然后与列向量形式的输入相乘："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://s3.cn-north-1.amazonaws.com.cn/u-img/b0d94818-c8f0-4c81-a573-d5fbf775220c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意：**\n",
    "\n",
    "这里权重的索引在上图中做了改变，与之前图片并不匹配。这是因为，在矩阵标注时行索引永远在列索引之前，所以用之前的方法做标识会引起误导。你只需要了解这跟之前的权重矩阵是一样的，只是做了转换，之前的第一列现在是第一行，之前的第二列现在是第二行。如果用**之前**的标记，权重矩阵是下面这个样子的：\n",
    "\n",
    "![](https://s3.cn-north-1.amazonaws.com.cn/u-img/06d40e12-57c1-4a5a-b697-c00eb1d22bd1)\n",
    "\n",
    "**用之前的标记标注的权重矩阵**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "切记，上面标注方式是**不正确**的，这里只是为了让你更清楚这个矩阵如何跟之前神经网络的权重匹配。\n",
    "\n",
    "矩阵相乘最重要的是他们的**维度相匹配**。因为它们在点乘时需要有相同数量的元素。在第一个例子中，输入向量有三列，权重矩阵有三行；第二个例子中，权重矩阵有三列，输入向量有三行。如果维度不匹配，你会得到：\n",
    "\n",
    "```python\n",
    "# Same weights and features as above, but swapped the order\n",
    "hidden_inputs = np.dot(weights_input_to_hidden, features)\n",
    "---------------------------------------------------------------------------\n",
    "ValueError                                Traceback (most recent call last)\n",
    "<ipython-input-11-1bfa0f615c45> in <module>()\n",
    "----> 1 hidden_in = np.dot(weights_input_to_hidden, X)\n",
    "\n",
    "ValueError: shapes (3,2) and (3,) not aligned: 2 (dim 1) != 3 (dim 0)\n",
    "```\n",
    "\n",
    "3x2 的矩阵跟 3 个元素的数组是没法相乘的。因为矩阵中的两列与数组中的元素个数并不匹配。能够相乘的矩阵如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://s3.cn-north-1.amazonaws.com.cn/u-img/72b656f0-00d7-4750-a182-0b354d1d9705)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里的规则是，如果是数组在左边，数组的元素个数必须与右边矩阵的行数一样。如果矩阵在左边，那么矩阵的列数，需要与右边向量的行数匹配。\n",
    "\n",
    "## 4.1 构建一个列向量\n",
    "\n",
    "看上面的介绍，你有时会需要一个列向量，尽管 NumPy 默认是行向量。你可以用 `arr.T` 来对数组进行转置，但对一维数组来说，转置还是行向量。所以你可以用 `arr[:,None]` 来创建一个列向量：\n",
    "\n",
    "```python\n",
    "print(features)\n",
    "> array([ 0.49671415, -0.1382643 ,  0.64768854])\n",
    "\n",
    "print(features.T)\n",
    "> array([ 0.49671415, -0.1382643 ,  0.64768854])\n",
    "\n",
    "print(features[:, None])\n",
    "> array([[ 0.49671415],\n",
    "       [-0.1382643 ],\n",
    "       [ 0.64768854]])\n",
    "```\n",
    "\n",
    "当然，你可以创建一个二维数组，然后用 `arr.T` 得到列向量。\n",
    "\n",
    "```python\n",
    "np.array(features, ndmin=2)\n",
    "> array([[ 0.49671415, -0.1382643 ,  0.64768854]])\n",
    "\n",
    "np.array(features, ndmin=2).T\n",
    "> array([[ 0.49671415],\n",
    "       [-0.1382643 ],\n",
    "       [ 0.64768854]])\n",
    "```\n",
    "\n",
    "我个人更倾向于保持所有向量为一维数组，这样可以更好认知。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 反向传播\n",
    "\n",
    "如何让多层神经网络学习呢？我们已了解了使用梯度下降来更新权重，反向传播算法则是它的一个延伸。以一个两层神经网络为例，可以使用 **链式法则** 计算输入层-隐藏层间权重的误差。\n",
    "\n",
    "要使用梯度下降法更新隐藏层的权重，你需要知道各隐藏层节点的误差对最终输出的影响。每层的输出是由两层间的权重决定的，两层之间产生的误差，按权重缩放后在网络中向前传播。既然我们知道输出误差，便可以用权重来反向传播到隐藏层。\n",
    "\n",
    "例如，输出层每个输出节点 $k$ 的误差是 $\\delta^o_k$，隐藏节点 $j$ 的误差即为输出误差乘以输出层-隐藏层间的权重矩阵（以及梯度）。\n",
    "\n",
    "![](https://s3.cn-north-1.amazonaws.com.cn/u-img/e8e8131d-1e97-4b07-b191-7945a68e26a8)\n",
    "\n",
    "然后，梯度下降与之前相同，只是用新的误差：\n",
    "\n",
    "![](https://s3.cn-north-1.amazonaws.com.cn/u-img/31b3a138-35c2-4987-afbe-cb052ff87d66)\n",
    "\n",
    "其中 $w_{ij}$ 是输入和隐藏层之间的权重， $x_i$ 是输入值。这个形式可以表示任意层数。权重更新步长等于步长乘以层输出误差再乘以该层的输入值。\n",
    "\n",
    "![](https://s3.cn-north-1.amazonaws.com.cn/u-img/0ef065b9-7e76-45f1-8c69-99177fc8bc9f)\n",
    "\n",
    "现在，你有了输出误差，$\\delta_{output}$，便可以反向传播这些误差了。$V_{in}$ 是该层的输入，比如经过隐藏层激活函数的输出值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 范例\n",
    "\n",
    "以一个简单的两层神经网络为例，计算其权重的更新过程。假设该神经网络包含两个输入值，一个隐藏节点和一个输出节点，隐藏层和输出层的激活函数都是 sigmoid，如下图所示。（注意：图底部的节点为输入值，图顶部的 $\\hat y$ 为输出值。输入层不计入层数，所以该结构被称为两层神经网络。）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://s3.cn-north-1.amazonaws.com.cn/u-img/0624caba-3895-48b9-8db1-8404983e4b33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设我们试着训练一些二进制数据，目标值是 $y = 1$。我们从正向传播开始，首先计算输入到隐藏层节点\n",
    "\n",
    "$h = \\sum_i w_i x_i = 0.1 \\times 0.4 - 0.2 \\times 0.3 = -0.02$\n",
    "\n",
    "以及隐藏层节点的输出\n",
    "\n",
    "$a = f(h) = \\mathrm{sigmoid}(-0.02) = 0.495$\n",
    "\n",
    "然后将其作为输出节点的输入，该神经网络的输出可表示为\n",
    "\n",
    "$\\hat y = f(W \\cdot a) = \\mathrm{sigmoid}(0.1 \\times 0.495) = 0.512$ \n",
    "\n",
    "基于该神经网络的**输出**，就可以使用反向传播来更新各层的权重了。sigmoid 函数的导数 $f'(W \\cdot a) = f(W \\cdot a) (1 - f(W \\cdot a))$，输出节点的误差项（error term）可表示为\n",
    "\n",
    "$\\delta^o = (y - \\hat y) f'(W \\cdot a) = (1 - 0.512) \\times 0.512 \\times(1 - 0.512) = 0.122$\n",
    "\n",
    "现在我们要通过反向传播来计算隐藏节点的误差项。这里我们把输出节点的误差项与隐藏层到输出层的权重 $W$ 相乘。隐藏节点的误差项 $\\delta^h_j = \\sum_k W_{jk} \\delta^o_k f'(h_j)$，因为该案例只有一个隐藏节点，这就比较简单了\n",
    "\n",
    "$\\delta^h = W \\delta^o f'(h) = 0.1 \\times 0.122 \\times 0.495 \\times (1 - 0.495) = 0.003$\n",
    "\n",
    "有了误差，就可以计算梯度下降步长了。隐藏层-输出层权重更新步长是学习速率乘以输出节点误差再乘以隐藏节点激活值。\n",
    "\n",
    "$\\Delta W = \\eta \\delta^o a = 0.5 \\times 0.122 \\times 0.495 = 0.0302$\n",
    "\n",
    "然后，输入-隐藏层权重 $w_i$ 是学习速率乘以隐藏节点误差再乘以输入值。\n",
    "\n",
    "$\\Delta w_i = \\eta \\delta^h x_i = (0.5 \\times 0.003 \\times 0.1, 0.5 \\times 0.003 \\times 0.3) = (0.00015, 0.00045)$\n",
    "\n",
    "从这个例子中你可以看到 sigmoid 做激活函数的一个缺点。sigmoid 函数导数的最大值是 0.25，因此输出层的误差被减少了至少 75%，隐藏层的误差被减少了至少 93.75%！如果你的神经网络有很多层，使用 sigmoid 激活函数会很快把靠近输入层的权重步长降为很小的值，该问题称作 **梯度消失**。后面的课程中你会学到在这方面表现更好，也被广泛用于最新神经网络中的其它激活函数。\n",
    "\n",
    "## 5.2 用 NumPy 来实现\n",
    "\n",
    "现在你已经有了大部分用 NumPy 来实现反向传播的知识。\n",
    "\n",
    "但是之前接触的只是单个节点的误差项。现在在更新权重时，我们需要考虑隐藏层 每个节点 的误差 $\\delta_j$：\n",
    "\n",
    "$\\Delta w_{ij} = \\eta \\delta_j x_i$\n",
    "\n",
    "首先，会有不同数量的输入和隐藏节点，所以试图把误差与输入当作行向量来乘会报错\n",
    "\n",
    "```python\n",
    "hidden_error*inputs\n",
    "---------------------------------------------------------------------------\n",
    "ValueError                                Traceback (most recent call last)\n",
    "<ipython-input-22-3b59121cb809> in <module>()\n",
    "----> 1 hidden_error*x\n",
    "\n",
    "ValueError: operands could not be broadcast together with shapes (3,) (6,)\n",
    "```\n",
    "\n",
    "另外，现在 $w_{ij}$ 是一个矩阵，所以右侧对应也应该有跟左侧同样的维度。幸运的是，NumPy 这些都能搞定。如果你用一个列向量数组和一个行向量数组相乘，它会把列向量的第一个元素与行向量的每个元素相乘，组成一个新的二维数组的第一行。列向量的每一个元素依次重复该过程，最后你会得到一个二维数组，形状是 (len(column_vector), len(row_vector))。\n",
    "\n",
    "```python\n",
    "hidden_error*inputs[:,None]\n",
    "array([[ -8.24195994e-04,  -2.71771975e-04,   1.29713395e-03],\n",
    "       [ -2.87777394e-04,  -9.48922722e-05,   4.52909055e-04],\n",
    "       [  6.44605731e-04,   2.12553536e-04,  -1.01449168e-03],\n",
    "       [  0.00000000e+00,   0.00000000e+00,  -0.00000000e+00],\n",
    "       [  0.00000000e+00,   0.00000000e+00,  -0.00000000e+00],\n",
    "       [  0.00000000e+00,   0.00000000e+00,  -0.00000000e+00]])\n",
    "```\n",
    "\n",
    "这正好是我们计算权重更新的步长的方式。跟以前一样，如果你的输入是一个一行的二维数组，你可以用 $hidden_error*inputs.T$，但是如果 $inputs$ 是一维数组，就不行了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 实现反向传播\n",
    "\n",
    "现在我们知道输出层的误差是\n",
    "\n",
    "$\\delta_k = (y_k - \\hat y_k) f'(a_k)$\n",
    "\n",
    "隐藏层误差是\n",
    "\n",
    "![](https://s3.cn-north-1.amazonaws.com.cn/u-img/95e33fe1-8ad2-4b9d-b10a-91466febfbca)\n",
    "\n",
    "现在我们只考虑一个简单神经网络，它只有一个隐藏层和一个输出节点。这是通过反向传播更新权重的算法概述：\n",
    "\n",
    "* 把每一层权重更新的初始步长设置为 0\n",
    "    * 输入到隐藏层的权重更新是 $\\Delta w_{ij} = 0$\n",
    "    * 隐藏层到输出层的权重更新是 $\\Delta W_j = 0$\n",
    "* 对训练数据当中的每一个点\n",
    "    * 让它正向通过网络，计算输出 $\\hat y$ \n",
    "    * 计算输出节点的误差梯度 $\\delta^o = (y - \\hat y) f'(z)$ 这里 $z = \\sum_j W_j a_j$ 是输出节点的输入。\n",
    "    * 误差传播到隐藏层 $\\delta^h_j = \\delta^o W_j f'(h_j)$\n",
    "    * 更新权重步长：\n",
    "        * $\\Delta W_j = \\Delta W_j + \\delta^o a_j$\n",
    "        * $\\Delta w_{ij} = \\Delta w_{ij} + \\delta^h_j a_i$\n",
    "* 更新权重, 其中 \\etaη 是学习率，mm 是数据点的数量：\n",
    "    * $W_j = W_j + \\eta \\Delta W_j / m$\n",
    "    * $w_{ij} = w_{ij} + \\eta \\Delta w_{ij} / m$\n",
    "* 重复这个过程 ee 代。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "[Cmd Markdown 公式指导手册](https://www.zybuluo.com/codeep/note/163962)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
