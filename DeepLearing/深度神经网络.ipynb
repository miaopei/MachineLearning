{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://s3.cn-north-1.amazonaws.com.cn/u-img/012a864a-3376-48c5-88be-8ca9fe31b1be)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 多层神经网络\n",
    "\n",
    "在本课程中，你会学到如何用 TensorFlow 构建多层神经网络。之前你应该了解，在网络里面添加一个隐藏层，可以让它构建更复杂的模型。而且，在隐藏层用非线性激活函数可以让它对非线性函数建模。\n",
    "\n",
    "一个常用的非线性函数叫 [ReLU（rectified linear unit）](https://en.wikipedia.org/wiki/Rectifier_(neural_networks))。ReLU 函数对所有负的输入，返回 0；所有 $x >0$ 的输入，返回 $x$。\n",
    "\n",
    "接下来你会看到如何在 TensorFlow 里实现一个 ReLU 隐藏层。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 TensorFlow ReLUs\n",
    "\n",
    "TensorFlow 提供了 ReLU 函数 `tf.nn.relu()`，如下所示：\n",
    "\n",
    "```shell\n",
    "# Hidden Layer with ReLU activation function\n",
    "# 隐藏层用 ReLU 作为激活函数\n",
    "hidden_layer = tf.add(tf.matmul(features, hidden_weights), hidden_biases)\n",
    "hidden_layer = tf.nn.relu(hidden_layer)\n",
    "\n",
    "output = tf.add(tf.matmul(hidden_layer, output_weights), output_biases)\n",
    "```\n",
    "\n",
    "上面的代码把 `tf.nn.relu()` 放到 `隐藏层`，就像开关一样把负权重关掉了。在激活函数之后，添加像 `输出层` 这样额外的层，就把模型变成了非线性函数。这个非线性的特征使得网络可以解决更复杂的问题。\n",
    "\n",
    "**练习：**\n",
    "\n",
    "下面你将用 ReLU 函数把一个线性单层网络转变成非线性多层网络。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://s3.cn-north-1.amazonaws.com.cn/u-img/06fd3fd3-9de8-4dfa-88fb-b806b0810065)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.11      8.440001]\n",
      " [ 0.        0.      ]\n",
      " [24.010002 38.239998]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "output = None\n",
    "hidden_layer_weights = [\n",
    "    [0.1, 0.2, 0.4],\n",
    "    [0.4, 0.6, 0.6],\n",
    "    [0.5, 0.9, 0.1],\n",
    "    [0.8, 0.2, 0.8]]\n",
    "out_weights = [\n",
    "    [0.1, 0.6],\n",
    "    [0.2, 0.1],\n",
    "    [0.7, 0.9]]\n",
    "\n",
    "# Weights and biases\n",
    "weights = [\n",
    "    tf.Variable(hidden_layer_weights),\n",
    "    tf.Variable(out_weights)]\n",
    "biases = [\n",
    "    tf.Variable(tf.zeros(3)),\n",
    "    tf.Variable(tf.zeros(2))]\n",
    "\n",
    "# Input\n",
    "features = tf.Variable([[1.0, 2.0, 3.0, 4.0], [-1.0, -2.0, -3.0, -4.0], [11.0, 12.0, 13.0, 14.0]])\n",
    "\n",
    "# TODO: Create Model\n",
    "hidden_layer = tf.add(tf.matmul(features, weights[0]), biases[0])\n",
    "hidden_layer = tf.nn.relu(hidden_layer)\n",
    "logits = tf.add(tf.matmul(hidden_layer, weights[1]), biases[1])\n",
    "\n",
    "# TODO: Print session results\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(logits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 TensorFlow 中的深度神经网络\n",
    "\n",
    "你已经学过了如何用 TensorFlow 构建一个逻辑分类器。现在你会学到如何用逻辑分类器来构建一个深度神经网络。\n",
    "\n",
    "### 1.2.1 详细指导\n",
    "\n",
    "接下来我们看看如何用 TensorFlow 来构建一个分类器来对 MNIST 数字进行分类。如果你要在自己电脑上跑这个代码，文件在[这儿](https://d17h27t6h515a5.cloudfront.net/topher/2017/February/58a61a3a_multilayer-perceptron/multilayer-perceptron.zip)。你可以在[Aymeric Damien 的 GitHub repository](https://github.com/aymericdamien/TensorFlow-Examples)里找到更多的 TensorFlow 的例子。\n",
    "\n",
    "### 1.2.2 代码\n",
    "\n",
    "**TensorFlow MNIST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-9e3367ff382d>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/miaopei/anaconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/miaopei/anaconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From /home/miaopei/anaconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From /home/miaopei/anaconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/miaopei/anaconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting ./t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting ./t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/miaopei/anaconda2/envs/dlnd-tf-lab/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\".\", one_hot=True, reshape=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你可以使用 TensorFlow 提供的 MNIST 数据集，他把分批和独热码都帮你处理好了。\n",
    "\n",
    "### 1.2.3 学习参数 Learning Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 参数 Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 20\n",
    "batch_size = 128  # 如果没有足够内存，可以降低 batch size\n",
    "display_step = 1\n",
    "\n",
    "n_input = 784  # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里的关注点是多层神经网络的架构，不是调参，所以这里直接给你了学习的参数。\n",
    "\n",
    "### 1.2.4 隐藏层参数 Hidden Layer Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_hidden_layer = 256 # layer number of features 特征的层数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`n_hidden_layer` 决定了神经网络隐藏层的大小。也被称作层的宽度。\n",
    "\n",
    "### 1.2.5 权重和偏置项 Weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "# 层权重和偏置项的储存\n",
    "weights = {\n",
    "    'hidden_layer': tf.Variable(tf.random_normal([n_input, n_hidden_layer])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_layer, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'hidden_layer': tf.Variable(tf.random_normal([n_hidden_layer])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "深度神经网络有多个层，每个层有自己的权重和偏置项。`'hidden_layer'` 的权重和偏置项只属于隐藏层（hidden_layer）， `'out'` 的权重和偏置项只属于输出层（output layer）。如果神经网络比这更深，那每一层都有权重和偏置项。\n",
    "\n",
    "### 1.2.6 输入 Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, 28, 28, 1])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "x_flat = tf.reshape(x, [-1, n_input])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST 数据集是由 28px * 28px 单[通道](https://en.wikipedia.org/wiki/Channel_(digital_image%29)图片组成。`tf.reshape()`函数把 28px * 28px 的矩阵转换成了 784px * 1px 的单行向量 `x`。\n",
    "\n",
    "### 1.2.7 多层感知器 Multilayer Perceptron\n",
    "\n",
    "![](https://s3.cn-north-1.amazonaws.com.cn/u-img/3dc523b3-ebb6-455d-a496-f2882c66ebe5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hidden layer with RELU activation\n",
    "# ReLU作为隐藏层激活函数\n",
    "layer_1 = tf.add(tf.matmul(x_flat, weights['hidden_layer']),\\\n",
    "    biases['hidden_layer'])\n",
    "layer_1 = tf.nn.relu(layer_1)\n",
    "# Output layer with linear activation\n",
    "# 输出层的线性激活函数\n",
    "logits = tf.add(tf.matmul(layer_1, weights['out']), biases['out'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你之前已经见过 `tf.add(tf.matmul(x_flat, weights['hidden_layer']), biases['hidden_layer'])`，也就是 `xw + b`。把线性函数与 ReLU 组合在一起，形成一个2层网络。\n",
    "\n",
    "### 1.2.8 优化器 Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-3fc89c6380dd>:3: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define loss and optimizer\n",
    "# 定义误差值和优化器\n",
    "cost = tf.reduce_mean(\\\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\\\n",
    "    .minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这跟 Intro to TensorFlow lab 里用到的优化技术一样。\n",
    "\n",
    "### 1.2.9 Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initializing the variables\n",
    "# 初始化变量\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "# 启动图\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    # Training cycle\n",
    "    # 训练循环\n",
    "    for epoch in range(training_epochs):\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        # 遍历所有 batch\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            # 运行优化器进行反向传导、计算 cost（获取 loss 值）\n",
    "            sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow 中的 MNIST 库提供了分批接收数据的能力。调用mnist.train.next_batch()函数返回训练数据的一个子集。\n",
    "\n",
    "### 1.2.10 深度神经网络\n",
    "\n",
    "![](https://s3.cn-north-1.amazonaws.com.cn/u-img/245d51c5-0167-4f6d-b80e-e71e126ebaca)\n",
    "\n",
    "就是这样！从一层到两层很简单。向网络中添加更多层，可以让你解决更复杂的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 保存和读取 TensorFlow 模型\n",
    "\n",
    "训练一个模型的时间很长。但是你一旦关闭了 TensorFlow session，你所有训练的权重和偏置项都丢失了。如果你计划在之后重新使用这个模型，你需要重新训练！\n",
    "\n",
    "幸运的是，TensorFlow 可以让你通过一个叫 `tf.train.Saver` 的类把你的进程保存下来。这个类可以把任何 `tf.Variable` 存到你的文件系统。\n",
    "\n",
    "### 1.3.1 保存变量\n",
    "让我们通过一个简单地例子来保存 `weights` 和 `bias` Tensors。第一个例子你只是存两个变量，后面会教你如何把一个实际模型的所有权重保存下来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights:\n",
      "[[ 0.9315782   0.28426078 -0.17783403]\n",
      " [ 1.10509     0.45378187 -0.22788237]]\n",
      "Bias:\n",
      "[-1.1493552   0.08015648 -0.96579224]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# The file path to save the data\n",
    "# 文件保存路径\n",
    "save_file = './model.ckpt'\n",
    "\n",
    "# Two Tensor Variables: weights and bias\n",
    "# 两个 Tensor 变量：权重和偏置项\n",
    "weights = tf.Variable(tf.truncated_normal([2, 3]))\n",
    "bias = tf.Variable(tf.truncated_normal([3]))\n",
    "\n",
    "# Class used to save and/or restore Tensor Variables\n",
    "# 用来存取 Tensor 变量的类\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initialize all the Variables\n",
    "    # 初始化所有变量\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Show the values of weights and bias\n",
    "   # 显示变量和权重\n",
    "    print('Weights:')\n",
    "    print(sess.run(weights))\n",
    "    print('Bias:')\n",
    "    print(sess.run(bias))\n",
    "\n",
    "    # Save the model\n",
    "    # 保存模型\n",
    "    saver.save(sess, save_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`weights` 和 `bias Tensors` 用 `tf.truncated_normal()` 函数设定了随机值。用 `tf.train.Saver.save()` 函数把这些值被保存在 `save_file` 位置，命名为 \"model.ckpt\"，（\".ckpt\" 扩展名表示\"checkpoint\"）。\n",
    "\n",
    "如果你使用 TensorFlow 0.11.0RC1 或者更新的版本，还会生成一个包含了 TensorFlow graph 的文件 \"model.ckpt.meta\"。\n",
    "\n",
    "### 1.3.2 加载变量\n",
    "\n",
    "现在这些变量已经存好了，让我们把它们加载到新模型里。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Remove the previous weights and bias\n",
    "# 移除之前的权重和偏置项\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Two Variables: weights and bias\n",
    "# 两个变量：权重和偏置项\n",
    "weights = tf.Variable(tf.truncated_normal([2, 3]))\n",
    "bias = tf.Variable(tf.truncated_normal([3]))\n",
    "\n",
    "# Class used to save and/or restore Tensor Variables\n",
    "# 用来存取 Tensor 变量的类\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Load the weights and bias\n",
    "    # 加载权重和偏置项\n",
    "    saver.restore(sess, save_file)\n",
    "\n",
    "    # Show the values of weights and bias\n",
    "    # 显示权重和偏置项\n",
    "    print('Weight:')\n",
    "    print(sess.run(weights))\n",
    "    print('Bias:')\n",
    "    print(sess.run(bias))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意，你依然需要在 Python 中创建 `weights` 和 `bias` Tensors。`tf.train.Saver.restore()` 函数把之前保存的数据加载到 `weights` 和 `bias` 当中。\n",
    "\n",
    "因为 `tf.train.Saver.restore()` 设定了 TensorFlow 变量，这里你不需要调用 `tf.global_variables_initializer()`了。\n",
    "\n",
    "### 1.3.3 保存一个训练好的模型\n",
    "\n",
    "让我们看看如何训练一个模型并保存它的权重。\n",
    "\n",
    "从一个模型开始："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./train-images-idx3-ubyte.gz\n",
      "Extracting ./train-labels-idx1-ubyte.gz\n",
      "Extracting ./t10k-images-idx3-ubyte.gz\n",
      "Extracting ./t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Remove previous Tensors and Operations\n",
    "# 移除之前的  Tensors 和运算\n",
    "tf.reset_default_graph()\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "\n",
    "learning_rate = 0.001\n",
    "n_input = 784  # MNIST 数据输入 (图片尺寸: 28*28)\n",
    "n_classes = 10  # MNIST 总计类别 (数字 0-9)\n",
    "\n",
    "# Import MNIST data\n",
    "# 加载 MNIST 数据\n",
    "mnist = input_data.read_data_sets('.', one_hot=True)\n",
    "\n",
    "# Features and Labels\n",
    "# 特征和标签\n",
    "features = tf.placeholder(tf.float32, [None, n_input])\n",
    "labels = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "# Weights & bias\n",
    "# 权重和偏置项\n",
    "weights = tf.Variable(tf.random_normal([n_input, n_classes]))\n",
    "bias = tf.Variable(tf.random_normal([n_classes]))\n",
    "\n",
    "# Logits - xW + b\n",
    "logits = tf.add(tf.matmul(features, weights), bias)\n",
    "\n",
    "# Define loss and optimizer\n",
    "# 定义损失函数和优化器\n",
    "cost = tf.reduce_mean(\\\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\\\n",
    "    .minimize(cost)\n",
    "\n",
    "# Calculate accuracy\n",
    "# 计算准确率\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们训练模型并保存权重："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0   - Validation Accuracy: 0.12160000205039978\n",
      "Epoch 10  - Validation Accuracy: 0.3003999888896942\n",
      "Epoch 20  - Validation Accuracy: 0.42719998955726624\n",
      "Epoch 30  - Validation Accuracy: 0.519599974155426\n",
      "Epoch 40  - Validation Accuracy: 0.5774000287055969\n",
      "Epoch 50  - Validation Accuracy: 0.6204000115394592\n",
      "Epoch 60  - Validation Accuracy: 0.6552000045776367\n",
      "Epoch 70  - Validation Accuracy: 0.6740000247955322\n",
      "Epoch 80  - Validation Accuracy: 0.6926000118255615\n",
      "Epoch 90  - Validation Accuracy: 0.7085999846458435\n",
      "Trained Model Saved.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "save_file = './train_model.ckpt'\n",
    "batch_size = 128\n",
    "n_epochs = 100\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Launch the graph\n",
    "# 启动图\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Training cycle\n",
    "    # 训练循环\n",
    "    for epoch in range(n_epochs):\n",
    "        total_batch = math.ceil(mnist.train.num_examples / batch_size)\n",
    "\n",
    "        # Loop over all batches\n",
    "        # 遍历所有 batch\n",
    "        for i in range(total_batch):\n",
    "            batch_features, batch_labels = mnist.train.next_batch(batch_size)\n",
    "            sess.run(\n",
    "                optimizer,\n",
    "                feed_dict={features: batch_features, labels: batch_labels})\n",
    "\n",
    "        # Print status for every 10 epochs\n",
    "        # 每运行10个 epoch 打印一次状态\n",
    "        if epoch % 10 == 0:\n",
    "            valid_accuracy = sess.run(\n",
    "                accuracy,\n",
    "                feed_dict={\n",
    "                    features: mnist.validation.images,\n",
    "                    labels: mnist.validation.labels})\n",
    "            print('Epoch {:<3} - Validation Accuracy: {}'.format(\n",
    "                epoch,\n",
    "                valid_accuracy))\n",
    "\n",
    "    # Save the model\n",
    "    # 保存模型\n",
    "    saver.save(sess, save_file)\n",
    "    print('Trained Model Saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.4 加载训练好的模型\n",
    "\n",
    "让我们从磁盘中加载权重和偏置项，验证测试集准确率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./train_model.ckpt\n",
      "Test Accuracy: 0.7294999957084656\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "# Launch the graph\n",
    "# 加载图\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, save_file)\n",
    "\n",
    "    test_accuracy = sess.run(\n",
    "        accuracy,\n",
    "        feed_dict={features: mnist.test.images, labels: mnist.test.labels})\n",
    "\n",
    "print('Test Accuracy: {}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "就是这样！你现在知道如何保存再加载一个 TensorFlow 的训练模型了。下一章节让我们看看如何把权重和偏置项加载到修改过的模型中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 把权重和偏置项加载到新模型中\n",
    "\n",
    "很多时候你想调整，或者说“微调”一个你已经训练并保存了的模型。但是，把保存的变量直接加载到已经修改过的模型会产生错误。让我们看看如何解决这个问题。\n",
    "\n",
    "### 1.4.1 命名报错\n",
    "\n",
    "TensorFlow 对 Tensor 和计算使用一个叫 `name` 的字符串辨识器，如果没有定义 `name`，TensorFlow 会自动创建一个。TensorFlow 会把第一个节点命名为 `<Type>`，把后续的命名为 `<Type>_<number>`。让我们看看这对加载一个有不同顺序权重和偏置项的模型有哪些影响："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Remove the previous weights and bias\n",
    "# 移除先前的权重和偏置项\n",
    "tf.reset_default_graph()\n",
    "\n",
    "save_file = 'model.ckpt'\n",
    "\n",
    "# Two Tensor Variables: weights and bias\n",
    "# 两个 Tensor 变量：权重和偏置项\n",
    "weights = tf.Variable(tf.truncated_normal([2, 3]))\n",
    "bias = tf.Variable(tf.truncated_normal([3]))\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Print the name of Weights and Bias\n",
    "# 打印权重和偏置项的名字\n",
    "print('Save Weights: {}'.format(weights.name))\n",
    "print('Save Bias: {}'.format(bias.name))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.save(sess, save_file)\n",
    "\n",
    "# Remove the previous weights and bias\n",
    "# 移除之前的权重和偏置项\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Two Variables: weights and bias\n",
    "# 两个变量：权重和偏置项\n",
    "bias = tf.Variable(tf.truncated_normal([3]))\n",
    "weights = tf.Variable(tf.truncated_normal([2, 3]))\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Print the name of Weights and Bias\n",
    "# 打印权重和偏置项的名字\n",
    "print('Load Weights: {}'.format(weights.name))\n",
    "print('Load Bias: {}'.format(bias.name))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Load the weights and bias - ERROR\n",
    "    # 加载权重和偏置项 - 报错\n",
    "    saver.restore(sess, save_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述代码会有下列输出：\n",
    "\n",
    "> Save Weights: Variable:0\n",
    "\n",
    "> Save Bias: Variable_1:0\n",
    "\n",
    "> Load Weights: Variable_1:0\n",
    "\n",
    "> Load Bias: Variable:0\n",
    "\n",
    "> ...\n",
    "\n",
    "> InvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match.\n",
    "\n",
    "> ...\n",
    "\n",
    "你注意到，`weights` 和 `bias` 的 `name` 属性与你保存的模型不同。这是为什么代码报“Assign requires shapes of both tensors to match”这个错误。`saver.restore(sess, save_file)` 代码试图把权重数据加载到bias里，把偏置项数据加载到 `weights`里。\n",
    "\n",
    "与其让 TensorFlow 来设定 `name` 属性，不如让我们来手动设定："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "save_file = 'model.ckpt'\n",
    "\n",
    "# Two Tensor Variables: weights and bias\n",
    "# 两个 Tensor 变量：权重和偏置项\n",
    "weights = tf.Variable(tf.truncated_normal([2, 3]), name='weights_0')\n",
    "bias = tf.Variable(tf.truncated_normal([3]), name='bias_0')\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Print the name of Weights and Bias\n",
    "# 打印权重和偏置项的名称\n",
    "print('Save Weights: {}'.format(weights.name))\n",
    "print('Save Bias: {}'.format(bias.name))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.save(sess, save_file)\n",
    "\n",
    "# Remove the previous weights and bias\n",
    "# 移除之前的权重和偏置项\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Two Variables: weights and bias\n",
    "# 两个变量：权重和偏置项\n",
    "bias = tf.Variable(tf.truncated_normal([3]), name='bias_0')\n",
    "weights = tf.Variable(tf.truncated_normal([2, 3]) ,name='weights_0')\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Print the name of Weights and Bias\n",
    "# 打印权重和偏置项的名称\n",
    "print('Load Weights: {}'.format(weights.name))\n",
    "print('Load Bias: {}'.format(bias.name))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Load the weights and bias - No Error\n",
    "    # 加载权重和偏置项 - 没有报错\n",
    "    saver.restore(sess, save_file)\n",
    "\n",
    "print('Loaded Weights and Bias successfully.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Save Weights: weights_0:0\n",
    "\n",
    "> Save Bias: bias_0:0\n",
    "\n",
    "> Load Weights: weights_0:0\n",
    "\n",
    "> Load Bias: bias_0:0\n",
    "\n",
    "> Loaded Weights and Bias successfully.\n",
    "\n",
    "这次没问题！Tensor 名称匹配正确，数据被正确加载。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 TensorFlow Dropout\n",
    "\n",
    "![](https://s3.cn-north-1.amazonaws.com.cn/u-img/47666c10-996f-4740-9674-ce54c6f768ae)\n",
    "\n",
    "(https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout 是一个降低过拟合的正则化技术。它在网络中暂时的丢弃一些单元（[神经元](https://en.wikipedia.org/wiki/Artificial_neuron)），以及与它们的前后相连的所有节点。图 1 是 dropout 的工作示意图。\n",
    "\n",
    "TensorFlow 提供了一个 `tf.nn.dropout()` 函数，你可以用来实现 dropout。\n",
    "\n",
    "让我们来看一个 `tf.nn.dropout()` 的使用例子。\n",
    "\n",
    "```python\n",
    "keep_prob = tf.placeholder(tf.float32) # probability to keep units\n",
    "\n",
    "hidden_layer = tf.add(tf.matmul(features, weights[0]), biases[0])\n",
    "hidden_layer = tf.nn.relu(hidden_layer)\n",
    "hidden_layer = tf.nn.dropout(hidden_layer, keep_prob)\n",
    "\n",
    "logits = tf.add(tf.matmul(hidden_layer, weights[1]), biases[1])\n",
    "```\n",
    "\n",
    "上面的代码展示了如何在神经网络中应用 dropout。\n",
    "\n",
    "`tf.nn.dropout()` 函数有两个参数：\n",
    "\n",
    "1. `hidden_layer`：你要应用 dropout 的 tensor\n",
    "2. `keep_prob`：任何一个给定单元的留存率（**没有**被丢弃的单元）\n",
    "\n",
    "`keep_prob` 可以让你调整丢弃单元的数量。为了补偿被丢弃的单元，`tf.nn.dropout()` 把所有保留下来的单元（**没有**被丢弃的单元）* `1/keep_prob`\n",
    "\n",
    "在训练时，一个好的 `keep_prob` 初始值是 `0.5`。\n",
    "\n",
    "在测试时，把 `keep_prob` 值设为 `1.0` ，这样保留所有的单元，最大化模型的能力。\n",
    "\n",
    "**练习**\n",
    "\n",
    "这个练习的代码来自 ReLU 的练习，应用一个 dropout 层。用 ReLU 层和 dropout 层构建一个模型，`keep_prob` 值设为 `0.5`。打印这个模型的 logits。\n",
    "\n",
    "注意: 由于 dropout 会随机丢弃单元，每次运行代码输出会有所不同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7.6799994  15.059999  ]\n",
      " [ 0.71400005  0.91800004]\n",
      " [ 9.56        4.78      ]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "hidden_layer_weights = [\n",
    "    [0.1, 0.2, 0.4],\n",
    "    [0.4, 0.6, 0.6],\n",
    "    [0.5, 0.9, 0.1],\n",
    "    [0.8, 0.2, 0.8]]\n",
    "out_weights = [\n",
    "    [0.1, 0.6],\n",
    "    [0.2, 0.1],\n",
    "    [0.7, 0.9]]\n",
    "\n",
    "# Weights and biases\n",
    "weights = [\n",
    "    tf.Variable(hidden_layer_weights),\n",
    "    tf.Variable(out_weights)]\n",
    "biases = [\n",
    "    tf.Variable(tf.zeros(3)),\n",
    "    tf.Variable(tf.zeros(2))]\n",
    "\n",
    "# Input\n",
    "features = tf.Variable([[0.0, 2.0, 3.0, 4.0], [0.1, 0.2, 0.3, 0.4], [11.0, 12.0, 13.0, 14.0]])\n",
    "\n",
    "# TODO: Create Model with Dropout\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "hidden_layer = tf.add(tf.matmul(features, weights[0]), biases[0])\n",
    "hidden_layer = tf.nn.relu(hidden_layer)\n",
    "hidden_layer = tf.nn.dropout(hidden_layer, keep_prob)\n",
    "\n",
    "logits = tf.add(tf.matmul(hidden_layer, weights[1]), biases[1])\n",
    "\n",
    "# TODO: Print logits from a session\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(logits, feed_dict={keep_prob: 0.5}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
